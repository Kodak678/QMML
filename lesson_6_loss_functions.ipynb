{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "56e0bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "70157c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(p, q):\n",
    "    distance = 0\n",
    "    for i in range(len(q)):\n",
    "        distance += (p[i] - q[i])**2\n",
    "    \n",
    "    return distance**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "20a346f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give example of iris data\n",
    "data = datasets.load_breast_cancer() # Loads a dict examine it\n",
    "#data = datasets.load_iris()\n",
    "\n",
    "X = data.data[:]\n",
    "y = data.target[:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "94c78c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "\n",
    "k = 5\n",
    "\n",
    "for test_feature in X_test:\n",
    "    distances = []\n",
    "    for x in X_train:        \n",
    "        distances.append(euclidean_distance(test_feature, x))\n",
    "\n",
    "    distances = np.argsort(distances)\n",
    "    k_nearest_neighbours = []\n",
    "    for i in distances[:k]:\n",
    "        k_nearest_neighbours.append(y_train[i])\n",
    "    \n",
    "    count_dict = {}\n",
    "    for i in k_nearest_neighbours:\n",
    "        if i in count_dict:\n",
    "            count_dict[i] += 1\n",
    "        else:\n",
    "            count_dict[i] = 1\n",
    "    \n",
    "    y_preds.append(max(count_dict, key=count_dict.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d0a0a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(0, 0)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in zip(y_preds, y_test):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790fbc82",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "85d2a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9590643274853801"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy = number of correct predictions / number of total predictions\n",
    "\n",
    "accuracy = np.sum(y_test == y_preds, axis=0) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "90bd2291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 89,   7],\n",
       "       [  7, 182]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix will tell us where we went wrong\n",
    "# TP FP TN FN\n",
    "\n",
    "num_classes = 2\n",
    "# num_classes = 3\n",
    "\n",
    "# init an nxn matrix with positions as classes\n",
    "confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "for i in range(len(y_preds)):\n",
    "    confusion_matrix[y_preds[i]][y_test[i]] += 1\n",
    "\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bef8ce",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP+FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160d3fdb",
   "metadata": {},
   "source": [
    "Precision tells us of the True positives we predicted how many did we predict correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fa74b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e209c1",
   "metadata": {},
   "source": [
    "Recall tells us of the total number of True positives how many did we identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7eaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
